---
layout: post
title: "The Bitter Lesson for Agentic AI: Why Simple Scales and Clever Breaks"
date: 2026-01-16 15:00:00 +0800
categories: [blog]
tags: [Agentic AI, github-pages]
---

# The Bitter Lesson for Agentic AI: Why Simple Scales and Clever Breaks

*January 2026*

When I started building my first agentic AI system, I thought I understood Richard Sutton's "The Bitter Lesson". I'd read it. I'd nodded along. I'd even cited it in discussions about the triumph of scale over hand-crafted features.

But I didn't *really* understand it until my carefully engineered agent crashed in production for the third time that week.

## What Sutton Actually Said

Let me recap Sutton's 2019 essay for those unfamiliar. His core argument:

> "The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin."

He provided compelling examples:
- **Computer chess**: Deep Blue's brute-force search crushed human chess knowledge
- **Computer Go**: AlphaGo's self-play learning demolished hand-crafted features
- **Speech recognition**: Statistical methods beat phonetic expertise
- **Computer vision**: Deep learning overtook carefully designed features

The pattern was consistent and humbling: **researchers kept trying to inject human knowledge into systems, and kept being outperformed by general methods that scaled with computation.**

The lesson was "bitter" because it meant abandoning intellectually satisfying work—elegant representations, domain expertise, clever heuristics—in favor of what seemed almost... dumb. Just throw more compute at it. Just let it learn.

## My Misunderstanding

Here's what I thought Sutton meant when I started building agentic AI:

*"Use big models, don't overthink it, scale solves everything."*

So I approached my first agent with confidence:
- Start with GPT-4 (big model ✓)
- Let it figure things out (general methods ✓)
- Don't over-engineer (avoid human knowledge ✓)

Then reality hit.

## The First Bitter Lesson: Simplicity ≠ Simplistic

My initial agent architecture was actually quite sophisticated:
- Complex state machine for conversation flow
- Elaborate prompt templates with 15+ examples
- Sophisticated retry logic with exponential backoff
- Carefully designed tool selection heuristics
- Multi-stage validation pipelines

It was elegant. It was clever. It was broken.

The agent would:
- Get stuck in states my state machine didn't anticipate
- Ignore my carefully crafted examples and hallucinate tools
- Retry the same failing action because my logic didn't account for certain error types
- Choose the wrong tool because my heuristics encoded my assumptions, not reality

**What actually worked:**

```python
# Instead of complex state machines
system_prompt = """You are an assistant that helps users accomplish tasks.
You have access to tools. Think step by step about what to do next."""

# Instead of 15 examples
# Just 2-3 clear, diverse examples

# Instead of sophisticated retry logic  
max_retries = 3  # Simple counter
if "error" in result:
    retry_with_error_context(result)

# Instead of tool selection heuristics
# Let the model choose based on clear tool descriptions
```

This felt too simple. Too naive. But it **worked reliably** while my clever version kept breaking.

**Sutton's lesson applying:** Don't encode your assumptions about how the agent *should* behave. Give it clear tools and let the model's general reasoning figure it out.

## The Second Bitter Lesson: Emergence Over Engineering

I built a research agent that needed to:
1. Search for information
2. Read multiple sources
3. Synthesize findings
4. Fact-check claims
5. Generate a report

My engineered approach:
```python
def research_workflow(query):
    # Step 1: Search strategy planning
    search_plan = plan_search_strategy(query)
    
    # Step 2: Execute searches
    sources = execute_search_plan(search_plan)
    
    # Step 3: Relevance filtering
    relevant = filter_by_relevance(sources, query)
    
    # Step 4: Synthesis pipeline
    synthesized = synthesis_pipeline(relevant)
    
    # Step 5: Fact-check validation
    validated = fact_check(synthesized)
    
    # Step 6: Report generation
    return generate_report(validated)
```

Each step had sub-functions, error handling, validation logic. It was a beautiful architecture diagram.

**What actually worked:**

```python
def research_workflow(query):
    result = agent.run(
        task=f"Research this topic and provide a comprehensive report: {query}",
        tools=[search, read_url, python_repl]
    )
    return result
```

That's it. The agent would:
- Search multiple times naturally as it discovered knowledge gaps
- Read sources it found relevant
- Re-search when it needed fact-checking
- Synthesize organically through its reasoning process

The sophisticated workflow I engineered? It was me encoding my human process. The agent's natural reasoning flow was often better—sometimes it fact-checked *before* full synthesis, sometimes it searched in different orders based on what it learned.

**Sutton's lesson applying:** Let general reasoning emerge. Don't hard-code your human algorithm.

## The Third Bitter Lesson: More Context Beats Clever Compression

I obsessed over prompt efficiency. Every token mattered. I built:
- Dynamic example selection (pick the 3 most relevant from a bank of 50)
- Compressed tool descriptions (abbreviate, remove redundancy)
- Conversation summarization (condense history every 5 turns)

This felt smart. Efficient. LLMs have context limits, right?

**Problems:**
- Dynamic examples sometimes picked edge cases that confused the model
- Compressed descriptions lost critical details (what does "proc" mean again?)
- Summarization occasionally dropped context that became crucial later

**What actually worked:**

Just include the full context:
- All tool descriptions (even if verbose)
- Full conversation history (until hitting actual limits)
- Complete examples (don't abbreviate)

When I did hit context limits, the solution wasn't clever compression—it was **better task decomposition** (split into subtasks) or **offloading to retrieval** (RAG for long-term memory).

**Sutton's lesson applying:** With large context windows, don't prematurely optimize. Let the model process full information. Scaling context is more effective than compressing cleverly.

## The Fourth Bitter Lesson: Iteration > Perfect First-Try

My engineering instinct: **make it work perfectly the first time.**

So I built:
- Elaborate input validation
- Pre-flight checks before tool calls
- Predictive error prevention
- Confidence scoring for outputs

**Reality:** The agent still made mistakes. But now it couldn't recover because my validation prevented certain valid paths, and my pre-checks added latency.

**What actually worked:**

```python
# Simple loop
for attempt in range(max_attempts):
    result = agent.step()
    
    if is_successful(result):
        return result
    
    # Give the agent its error and let it try again
    context.add_error(result.error)
```

The agent got *really good* at:
- Recognizing its mistakes
- Adjusting its approach
- Learning from error messages
- Trying alternative strategies

My perfect-first-try engineering? It prevented the agent from developing this resilience.

**Sutton's lesson applying:** Agents learn through trial and error. Don't prevent failures—enable recovery. Iteration beats prediction.

## The Fifth Bitter Lesson: Data Quality > Prompt Wizardry

I spent weeks perfecting prompts:
- A/B testing phrasing
- Optimizing instruction order
- Fine-tuning tone and style
- Crafting the perfect system message

Meanwhile, my tool descriptions were inconsistent, examples had errors, and documentation was unclear.

**The breakthrough:** I spent two days cleaning up:
- Tool descriptions (clear, consistent format)
- Examples (accurate, diverse, debugged)
- Error messages (informative, actionable)

My mediocre prompts with clean data **vastly outperformed** my optimized prompts with messy data.

**Sutton's lesson applying:** Clean, abundant data beats clever prompting. Every hour spent on prompt engineering should probably be two hours on data quality.

## What This Means for Building Agentic AI

Sutton's bitter lesson wasn't "don't think, just scale." It was: **our human intuitions about how to build intelligent systems are often wrong. General methods that leverage learning and computation work better than encoding our clever ideas.**

For agentic AI, this translates to:

### 1. **Trust the Model's Reasoning**
Don't encode your algorithm. Provide clear tools and objectives. Let reasoning emerge.

### 2. **Simplify Your Architecture**
Every hard-coded decision is a bet that you know better than the model. You probably don't.

### 3. **Invest in Foundations, Not Clever Tricks**
- Clear tool interfaces > sophisticated orchestration
- Good examples > perfect prompt engineering
- Simple retry logic > predictive error prevention
- Full context > clever compression

### 4. **Enable Learning Loops**
- Let agents iterate and fail
- Provide informative errors
- Allow course correction
- Don't prevent recovery

### 5. **Scale What Works**
- More examples beats few-shot cleverness
- Bigger context beats summarization hacks
- More attempts beats perfect first try

## The Hardest Part

The bitter part of these lessons isn't that they're technically difficult. It's that they're *emotionally* difficult.

As engineers, we want to solve problems with our intelligence. We want to:
- Design elegant architectures
- Craft clever solutions
- Encode our expertise
- Optimize and perfect

But building effective agentic AI often means:
- Deleting code we're proud of
- Choosing simple over clever
- Admitting the model knows better
- Letting emergence replace engineering

This requires humility. It requires trusting that general methods—clear interfaces, good data, iterative learning—will outperform our specific cleverness.

## Looking Forward

I still make these mistakes. I still catch myself:
- Over-engineering workflows
- Optimizing prompts instead of fixing data
- Preventing failures instead of enabling recovery
- Encoding assumptions instead of letting patterns emerge

But now I recognize them faster. And when I do, I remember Sutton's lesson: **70 years of AI research showed that betting against general methods was always wrong.**

Why would agentic AI be different?

## Your Bitter Lessons

These are my lessons from my systems. Yours will be different—different domains, different constraints, different failures.

But I'd bet they'll follow the same pattern: your clever ideas didn't work as well as you hoped, and the simple, general approach you resisted worked better than you expected.

That's the bitter lesson.

---

**Next in this series:** "The Tool Use Fallacy: Why Your Elegant API Design Makes Agents Worse"

**What bitter lessons have you learned building agentic AI? I'd love to hear about the clever things you deleted and the simple things that worked.** [stone.chenlei@gmail.com]
